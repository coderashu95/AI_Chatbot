{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Overview</h1>\n",
        "\n",
        "This script creates a web application called \"Document Parser\" using Streamlit. It allows users to upload PDF documents, enter a Google API key, and ask questions about the content of the documents. The application processes the documents, splits the text into manageable chunks, converts them into embeddings, and then uses these embeddings to answer user queries.\n",
        "\n",
        "<h1>Code Breakdown</h1>\n",
        "\n",
        "Install the necessary libraries before importing them.\n",
        "\n",
        "1. **Importing Libraries**:\n",
        "\n",
        "    streamlit is used to create the web interface.\n",
        "    os for interacting with the operating system.\n",
        "    langchain.text_splitter to split large text into smaller chunks.\n",
        "    langchain.vectorstores for storing text chunks in a vector database.\n",
        "    langchain.chains.question_answering to create a question-answering chain.\n",
        "    langchain.prompts for defining prompt templates.\n",
        "    PyPDF2 to read PDF files.\n",
        "    langchain_google_genai for embeddings and chat models using Google Generative AI.\n",
        "\n",
        "2. **Streamlit Page Configuration**:\n",
        "\n",
        "    Sets the page title and layout.\n",
        "    Displays instructions on how to use the application.\n",
        "\n",
        "3. **API Key Input**:\n",
        "\n",
        "    A secure input field for the user to enter their Google API key.\n",
        "\n",
        "4. **Extracting Text from PDFs**:\n",
        "\n",
        "    get_pdf_text function reads text from uploaded PDF files.\n",
        "\n",
        "5. **Splitting Text into Chunks**:\n",
        "\n",
        "    get_text_chunks function splits the extracted text into smaller chunks for easier processing.\n",
        "\n",
        "6. **Creating a Vector Store**:\n",
        "\n",
        "    get_vector_store function converts text chunks into embeddings and stores them in a vector database.\n",
        "\n",
        "7. **Creating a Conversational Chain**:\n",
        "\n",
        "    get_conversational_chain function sets up a model to handle user queries by providing detailed answers based on the context from the document.\n",
        "\n",
        "8. **Handling User Input**:\n",
        "\n",
        "    user_input function processes the user's question, searches for relevant document chunks, and generates a response using the conversational chain.\n",
        "\n",
        "9. **Main Function**:\n",
        "\n",
        "    Sets up the Streamlit interface, including a header, input fields for user questions, and a sidebar menu for uploading PDFs.\n",
        "    Processes the documents and prepares them for question-answering when the user clicks the \"Submit & Process\" button.\n",
        "\n",
        "<h1>How it Works</h1>\n",
        "\n",
        "User Interaction:\n",
        "\n",
        "    User enters their Google API key.\n",
        "    User uploads PDF documents.\n",
        "    User asks a question about the content of the documents.\n",
        "\n",
        "Document Processing:\n",
        "\n",
        "    Text is extracted from the PDFs.\n",
        "    The text is split into smaller chunks.\n",
        "    These chunks are converted into embeddings and stored in a vector database.\n",
        "\n",
        "Question Answering:\n",
        "\n",
        "    When the user asks a question, the application searches the vector database for relevant text chunks.\n",
        "    The conversational chain model generates a detailed answer based on the context provided by these chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "So6Pt7n6CkUl",
        "outputId": "9b39c2e3-8e92-4824-ba78-16b2488c0f26"
      },
      "outputs": [],
      "source": [
        "#!pip install streamlit\n",
        "#!pip install langchain_google_genai\n",
        "#!pip install google.generativeai\n",
        "#!pip install -U langchain-community\n",
        "#!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzZHpSEYDbLQ",
        "outputId": "0863b40a-6c6e-439e-f1c6-af98cc092418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st  # Used to create interactive web applications using Python\n",
        "import os\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter #SplittingTextIntoSmallerChunks\n",
        "from langchain_community.vectorstores import FAISS #VectorDatabase\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings #UsedToConverChunksIntoPositionalEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.api_core import retry\n",
        "import google.generativeai as genai\n",
        "import time\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "st.set_page_config(page_title=\"Personal Document Chatbot Tool\", layout=\"wide\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "## Document Chatbot: Get instant insights from your document\n",
        "## How it starts:\n",
        "\n",
        "Follow the instructions as below:\n",
        "\n",
        "1. **Enter API Key** - You will need Google API key for Chatbot to access Google's generative AI models. Obtain your API key here : https://developers.google.com/maps/documentation/javascript/get-api-key#create-api-keys \\n\n",
        "2. **Upload your documents** - System accepts multiple PDF files at once, analyzing the content to provide comprehensive insights.\n",
        "3. **Ask a question** - After getting \"Processing Done!\" message, ask any question related to the content of your uploaded documents.\n",
        "\"\"\")\n",
        "\n",
        "api_key = st.text_input(\"Enter your Google API Key:\", type=\"password\", key=\"api_key_input\")\n",
        "\n",
        "# This is a function to extract text from uploaded PDFs\n",
        "def get_pdf_text(pdf_docs):\n",
        "    text = \"\"\n",
        "\n",
        "    # Code has been modified to take care of error handling\n",
        "    for pdf in pdf_docs:\n",
        "        try:\n",
        "            pdf_reader = PdfReader(pdf)\n",
        "            for page in pdf_reader.pages:\n",
        "                text += page.extract_text()\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error reading {pdf.name}: {e}\")\n",
        "            continue\n",
        "    if text:\n",
        "        st.info(\"Text extraction complete.\") \n",
        "    else:\n",
        "        st.warning(\"No text extracted. Please check your PDF files.\")\n",
        "    return text\n",
        "\n",
        "# This is a function to split pdf text to text chunks\n",
        "def get_text_chunks(text):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "# This is a function to convert text chunks into positional embeddings, and store those embeddings into vector database\n",
        "def get_vector_store(text_chunks, api_key):\n",
        "    \n",
        "    # Modified code to handle timeout issues while creating embeddings \n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key, request_options={'retry':retry.Retry()})\n",
        "    vector_store = None\n",
        "    retry_attempts = 3\n",
        "    for attempt in range(retry_attempts):\n",
        "        try:\n",
        "            vector_store = FAISS.from_texts(text_chunks, embeddings)\n",
        "            vector_store.save_local(\"faiss_index\")\n",
        "            st.success(\"Embeddings created and saved successfully.\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error embedding content: {e}\", exc_info=True)\n",
        "            st.error(f\"Error embedding content: {e}. Retrying... ({attempt + 1}/{retry_attempts})\")\n",
        "            time.sleep(2 * (attempt + 1))\n",
        "            continue\n",
        "    if vector_store is None:\n",
        "        st.error(\"Failed to create embeddings after multiple attempts.\")\n",
        "\n",
        "def load_vector_store(api_key):\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key)\n",
        "    vector_store = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "    return vector_store\n",
        "\n",
        "# This function will allow user to see the chain of conversation for a particular topic and help build on context for more accurate answers\n",
        "def get_conversational_chain(api_key):\n",
        "    \n",
        "    #This is where prompt engineering happens. It is crucial to get this right as the length and articluation of prompt affects response quality.\n",
        "    prompt_template = \"\"\"\n",
        "    Answer the question as precise as possible from the provided context.\n",
        "    Make sure to provide all the details. If the answer is not available, don't hallucinate and say \"Answer not available in context\" \\n\\n.\n",
        "    \n",
        "    Context: \\n {context}?\\n\n",
        "    Question: \\n{question}\\n\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3, google_api_key=api_key)\n",
        "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "    chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
        "    return chain\n",
        "\n",
        "def user_input(user_question, api_key):\n",
        "    vector_store = load_vector_store(api_key)\n",
        "    docs = vector_store.similarity_search(user_question)\n",
        "    chain = get_conversational_chain(api_key)\n",
        "    response = chain({\"input_documents\": docs, \"question\": user_question}, return_only_outputs=True)\n",
        "    st.write(\"Reply: \", response[\"output_text\"])\n",
        "\n",
        "# This snippet will be responsible for setting up the Streamlit page\n",
        "def main():\n",
        "    st.header(\"Document Chatbot Panel:\")\n",
        "\n",
        "    user_question = st.text_input(\"Ask a question from uploaded PDF files\", key=\"user_question\")\n",
        "\n",
        "    if user_question and api_key:\n",
        "        user_input(user_question, api_key)\n",
        "\n",
        "    with st.sidebar:\n",
        "        st.title(\"Menu:\")\n",
        "        pdf_docs = st.file_uploader(\"Upload your documents\", accept_multiple_files=True, key=\"pdf_uploader\")\n",
        "        if st.button(\"Submit & Process\", key=\"process_button\") and api_key:\n",
        "            with st.spinner(\"Processing...\"):\n",
        "                try:\n",
        "                    raw_text = get_pdf_text(pdf_docs)\n",
        "                    text_chunks = get_text_chunks(raw_text)\n",
        "                    get_vector_store(text_chunks, api_key)\n",
        "                    st.success(\"Processing Done!\")\n",
        "                except Exception as e:\n",
        "                    st.error(f\"An error occurred during processing: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After executing the above code, app.py file should be generated in your project folder. Use the terminal to navigate to the project folder containing app.py file using cd \"{folder full path}\" and then execute the command \"streamlit run app.py\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
